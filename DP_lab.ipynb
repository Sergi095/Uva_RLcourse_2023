{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Dynamic Programming\n",
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dp_autograde.py file into codegrade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports %%execwritefile command (executes cell and writes it into file). \n",
    "# All cells that start with %%execwritefile should be in dp_autograde.py file after running all cells.\n",
    "from custommagics import CustomMagics\n",
    "get_ipython().register_magics(CustomMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dp_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile dp_autograde.py\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Policy Evaluation (1 point)\n",
    "In this exercise we will evaluate a policy, e.g. find the value function of a policy. The problem we consider is the gridworld from Example 4.1 in the book. The environment is implemented as `GridworldEnv`, which is a subclass of the `Env` class from [OpenAI Gym](https://github.com/openai/gym). This means that we can interact with the environment. We can look at the documentation to see how we can interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        GridworldEnv\n",
      "\u001b[1;31mString form:\u001b[0m <GridworldEnv instance>\n",
      "\u001b[1;31mFile:\u001b[0m        d:\\sergio\\github_repos\\uva_rlcourse_2023\\gridworld.py\n",
      "\u001b[1;31mDocstring:\u001b[0m  \n",
      "Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n",
      "You are an agent on an MxN grid and your goal is to reach the terminal\n",
      "state at the top left or the bottom right corner.\n",
      "\n",
      "For example, a 4x4 grid looks as follows:\n",
      "\n",
      "T  o  o  o\n",
      "o  x  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "\n",
      "x is your position and T are the two terminal states.\n",
      "\n",
      "You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
      "Actions going off the edge leave you in your current state.\n",
      "You receive a reward of -1 at each step until you reach a terminal state.\n"
     ]
    }
   ],
   "source": [
    "from gridworld import GridworldEnv\n",
    "env = GridworldEnv()\n",
    "# Lets see what this is\n",
    "?env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        GridworldEnv\n",
      "\u001b[1;31mString form:\u001b[0m <GridworldEnv instance>\n",
      "\u001b[1;31mFile:\u001b[0m        d:\\sergio\\github_repos\\uva_rlcourse_2023\\gridworld.py\n",
      "\u001b[1;31mSource:\u001b[0m     \n",
      "\u001b[1;32mclass\u001b[0m \u001b[0mGridworldEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscrete\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDiscreteEnv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"\"\"\n",
      "    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n",
      "    You are an agent on an MxN grid and your goal is to reach the terminal\n",
      "    state at the top left or the bottom right corner.\n",
      "\n",
      "    For example, a 4x4 grid looks as follows:\n",
      "\n",
      "    T  o  o  o\n",
      "    o  x  o  o\n",
      "    o  o  o  o\n",
      "    o  o  o  T\n",
      "\n",
      "    x is your position and T are the two terminal states.\n",
      "\n",
      "    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
      "    Actions going off the edge leave you in your current state.\n",
      "    You receive a reward of -1 at each step until you reach a terminal state.\n",
      "    \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'render.modes'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ansi'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'shape argument must be a list/tuple of length 2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mnS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mnA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mMAX_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mMAX_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'multi_index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinished\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterindex\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_index\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mis_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnS\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# We're stuck in a terminal state\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUP\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRIGHT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDOWN\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLEFT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# Not a terminal state\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mns_up\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mMAX_X\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mns_right\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mMAX_X\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mns_down\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mMAX_Y\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mMAX_X\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mns_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUP\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns_up\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mns_up\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRIGHT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns_right\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mns_right\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDOWN\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns_down\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mns_down\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLEFT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mns_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miternext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Initial state distribution is uniform\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0misd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnS\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# We expose the model of the environment for educational purposes\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# This should not be used in any model-free learning algorithm\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGridworldEnv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mreturn\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0moutfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'ansi'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'multi_index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinished\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterindex\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_index\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" x \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnS\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" T \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" o \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0moutfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0moutfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miternext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# To have a quick look into the code\n",
    "??env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to evaluate a policy by using Dynamic Programming. For more information, see the [Intro to RL](https://drive.google.com/open?id=1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG) book, section 4.1. This algorithm requires knowledge of the problem dynamics in the form of the transition probabilities $p(s',r|s,a)$. In general these are not available, but for our gridworld we know the dynamics and these can be accessed as `env.P`. Note that we do not need to use a discount_factor for episodic tasks but make sure your implementation can handle this correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, True)],\n",
       "  1: [(1.0, 0, 0.0, True)],\n",
       "  2: [(1.0, 0, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, True)]},\n",
       " 1: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 2, -1.0, False)],\n",
       "  2: [(1.0, 5, -1.0, False)],\n",
       "  3: [(1.0, 0, -1.0, True)]},\n",
       " 2: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 6, -1.0, False)],\n",
       "  3: [(1.0, 1, -1.0, False)]},\n",
       " 3: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 7, -1.0, False)],\n",
       "  3: [(1.0, 2, -1.0, False)]},\n",
       " 4: {0: [(1.0, 0, -1.0, True)],\n",
       "  1: [(1.0, 5, -1.0, False)],\n",
       "  2: [(1.0, 8, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 5: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 6, -1.0, False)],\n",
       "  2: [(1.0, 9, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 6: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 10, -1.0, False)],\n",
       "  3: [(1.0, 5, -1.0, False)]},\n",
       " 7: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 11, -1.0, False)],\n",
       "  3: [(1.0, 6, -1.0, False)]},\n",
       " 8: {0: [(1.0, 4, -1.0, False)],\n",
       "  1: [(1.0, 9, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 9: {0: [(1.0, 5, -1.0, False)],\n",
       "  1: [(1.0, 10, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 10: {0: [(1.0, 6, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 9, -1.0, False)]},\n",
       " 11: {0: [(1.0, 7, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 15, -1.0, True)],\n",
       "  3: [(1.0, 10, -1.0, False)]},\n",
       " 12: {0: [(1.0, 8, -1.0, False)],\n",
       "  1: [(1.0, 13, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 13: {0: [(1.0, 9, -1.0, False)],\n",
       "  1: [(1.0, 14, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 14: {0: [(1.0, 10, -1.0, False)],\n",
       "  1: [(1.0, 15, -1.0, True)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 13, -1.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0.0, True)],\n",
       "  1: [(1.0, 15, 0.0, True)],\n",
       "  2: [(1.0, 15, 0.0, True)],\n",
       "  3: [(1.0, 15, 0.0, True)]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a moment to figure out what P represents. \n",
    "# Note that this is a deterministic environment. \n",
    "# What would a stochastic environment look like?\n",
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dp_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def policy_eval_v(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with an all 0 value function\n",
    "    V = np.zeros(env.nS)\n",
    "    # print(env.nS)\n",
    "    # YOUR CODE HERE\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(env.nS):\n",
    "            v_old = V[state]\n",
    "            v_new = 0.0\n",
    "            for action, action_prob in enumerate(policy[state]):\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    v_new += action_prob * (prob * (reward + discount_factor * V[next_state]))\n",
    "            V[state] = v_new\n",
    "            delta = max(delta, np.abs(v_new - v_old))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -13.99993529, -19.99990698, -21.99989761,\n",
       "       -13.99993529, -17.9999206 , -19.99991379, -19.99991477,\n",
       "       -19.99990698, -19.99991379, -17.99992725, -13.99994569,\n",
       "       -21.99989761, -19.99991477, -13.99994569,   0.        ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run your code, does it make sense?\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "V = policy_eval_v(random_policy, env)\n",
    "assert V.shape == (env.nS)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_gridworld_value(V):\n",
    "    plt.figure()\n",
    "    c = plt.pcolormesh(V, cmap='gray')\n",
    "    plt.colorbar(c)\n",
    "    plt.gca().invert_yaxis()  # In the array, first row = 0 is on top\n",
    "\n",
    "# Making a plot always helps\n",
    "plot_gridworld_value(V.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Policy Iteration (2 points)\n",
    "Using the policy evaluation algorithm we can implement policy iteration to find a good policy for this problem. Note that we do not need to use a discount_factor for episodic tasks but make sure your implementation can handle this correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def policy_iter_v(env, policy_eval_v=policy_eval_v, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Iteration Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_v: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    # raise NotImplementedError\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what it does\n",
    "policy, v = policy_iter_v(env, policy_eval_v)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "def print_grid_policy(policy, symbols=[\"^\", \">\", \"v\", \"<\"]):\n",
    "    symbols = np.array(symbols)\n",
    "    for row in policy:\n",
    "        print(\"\".join(symbols[row]))\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "plot_gridworld_value(v.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Q-value Iteration (3 points)\n",
    "In this exercise you will implement the value iteration algorithm. However, because this algorithm is quite similar to the ones you implemented previously, we will spice things up a bit and use Q-values instead. Thus instead of using Bellman optimality equations for V you will use Bellman equations for Q. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def value_iter_q(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Q-value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all state-action pairs.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, Q) of the optimal policy and the optimal Q-value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with an all 0 Q-value function\n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what it does\n",
    "policy, Q = value_iter_q(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Q Function:\")\n",
    "print(Q)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, the visualization of the Q function is quite clumsy and is not that easy to check \n",
    "# that all values make sense. However, you can easily create a V function from Q and policy to double\n",
    "# check that the values are what you would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dp_autograde.py file into codegrade.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
