{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Dynamic Programming\n",
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dp_autograde.py file into codegrade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports %%execwritefile command (executes cell and writes it into file). \n",
    "# All cells that start with %%execwritefile should be in dp_autograde.py file after running all cells.\n",
    "from custommagics import CustomMagics\n",
    "get_ipython().register_magics(CustomMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dp_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile dp_autograde.py\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Policy Evaluation (1 point)\n",
    "In this exercise we will evaluate a policy, e.g. find the value function of a policy. The problem we consider is the gridworld from Example 4.1 in the book. The environment is implemented as `GridworldEnv`, which is a subclass of the `Env` class from [OpenAI Gym](https://github.com/openai/gym). This means that we can interact with the environment. We can look at the documentation to see how we can interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        GridworldEnv\n",
      "\u001b[1;31mString form:\u001b[0m <GridworldEnv instance>\n",
      "\u001b[1;31mFile:\u001b[0m        d:\\sergio\\github_repos\\uva_rlcourse_2023\\gridworld.py\n",
      "\u001b[1;31mDocstring:\u001b[0m  \n",
      "Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n",
      "You are an agent on an MxN grid and your goal is to reach the terminal\n",
      "state at the top left or the bottom right corner.\n",
      "\n",
      "For example, a 4x4 grid looks as follows:\n",
      "\n",
      "T  o  o  o\n",
      "o  x  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "\n",
      "x is your position and T are the two terminal states.\n",
      "\n",
      "You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
      "Actions going off the edge leave you in your current state.\n",
      "You receive a reward of -1 at each step until you reach a terminal state.\n"
     ]
    }
   ],
   "source": [
    "from gridworld import GridworldEnv\n",
    "env = GridworldEnv()\n",
    "# Lets see what this is\n",
    "?env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        GridworldEnv\n",
      "\u001b[1;31mString form:\u001b[0m <GridworldEnv instance>\n",
      "\u001b[1;31mFile:\u001b[0m        d:\\sergio\\github_repos\\uva_rlcourse_2023\\gridworld.py\n",
      "\u001b[1;31mSource:\u001b[0m     \n",
      "\u001b[1;32mclass\u001b[0m \u001b[0mGridworldEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscrete\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDiscreteEnv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"\"\"\n",
      "    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n",
      "    You are an agent on an MxN grid and your goal is to reach the terminal\n",
      "    state at the top left or the bottom right corner.\n",
      "\n",
      "    For example, a 4x4 grid looks as follows:\n",
      "\n",
      "    T  o  o  o\n",
      "    o  x  o  o\n",
      "    o  o  o  o\n",
      "    o  o  o  T\n",
      "\n",
      "    x is your position and T are the two terminal states.\n",
      "\n",
      "    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
      "    Actions going off the edge leave you in your current state.\n",
      "    You receive a reward of -1 at each step until you reach a terminal state.\n",
      "    \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'render.modes'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ansi'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'shape argument must be a list/tuple of length 2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mnS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mnA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mMAX_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mMAX_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'multi_index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinished\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterindex\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_index\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mis_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnS\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# We're stuck in a terminal state\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUP\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRIGHT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDOWN\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLEFT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;31m# Not a terminal state\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mns_up\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mMAX_X\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mns_right\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mMAX_X\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mns_down\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mMAX_Y\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mMAX_X\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mns_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUP\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns_up\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mns_up\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRIGHT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns_right\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mns_right\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDOWN\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns_down\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mns_down\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLEFT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mns_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miternext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Initial state distribution is uniform\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0misd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnS\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# We expose the model of the environment for educational purposes\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# This should not be used in any model-free learning algorithm\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGridworldEnv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mreturn\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0moutfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'ansi'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'multi_index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinished\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterindex\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_index\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" x \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnS\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" T \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" o \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0moutfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0moutfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miternext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# To have a quick look into the code\n",
    "??env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to evaluate a policy by using Dynamic Programming. For more information, see the [Intro to RL](https://drive.google.com/open?id=1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG) book, section 4.1. This algorithm requires knowledge of the problem dynamics in the form of the transition probabilities $p(s',r|s,a)$. In general these are not available, but for our gridworld we know the dynamics and these can be accessed as `env.P`. Note that we do not need to use a discount_factor for episodic tasks but make sure your implementation can handle this correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, True)],\n",
       "  1: [(1.0, 0, 0.0, True)],\n",
       "  2: [(1.0, 0, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, True)]},\n",
       " 1: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 2, -1.0, False)],\n",
       "  2: [(1.0, 5, -1.0, False)],\n",
       "  3: [(1.0, 0, -1.0, True)]},\n",
       " 2: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 6, -1.0, False)],\n",
       "  3: [(1.0, 1, -1.0, False)]},\n",
       " 3: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 7, -1.0, False)],\n",
       "  3: [(1.0, 2, -1.0, False)]},\n",
       " 4: {0: [(1.0, 0, -1.0, True)],\n",
       "  1: [(1.0, 5, -1.0, False)],\n",
       "  2: [(1.0, 8, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 5: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 6, -1.0, False)],\n",
       "  2: [(1.0, 9, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 6: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 10, -1.0, False)],\n",
       "  3: [(1.0, 5, -1.0, False)]},\n",
       " 7: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 11, -1.0, False)],\n",
       "  3: [(1.0, 6, -1.0, False)]},\n",
       " 8: {0: [(1.0, 4, -1.0, False)],\n",
       "  1: [(1.0, 9, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 9: {0: [(1.0, 5, -1.0, False)],\n",
       "  1: [(1.0, 10, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 10: {0: [(1.0, 6, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 9, -1.0, False)]},\n",
       " 11: {0: [(1.0, 7, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 15, -1.0, True)],\n",
       "  3: [(1.0, 10, -1.0, False)]},\n",
       " 12: {0: [(1.0, 8, -1.0, False)],\n",
       "  1: [(1.0, 13, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 13: {0: [(1.0, 9, -1.0, False)],\n",
       "  1: [(1.0, 14, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 14: {0: [(1.0, 10, -1.0, False)],\n",
       "  1: [(1.0, 15, -1.0, True)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 13, -1.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0.0, True)],\n",
       "  1: [(1.0, 15, 0.0, True)],\n",
       "  2: [(1.0, 15, 0.0, True)],\n",
       "  3: [(1.0, 15, 0.0, True)]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a moment to figure out what P represents. \n",
    "# Note that this is a deterministic environment. \n",
    "# What would a stochastic environment look like?\n",
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dp_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def policy_eval_v(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with an all 0 value function\n",
    "    V = np.zeros(env.nS)\n",
    "    # YOUR CODE HERE\n",
    "    delta = 0\n",
    "    for s in range(env.nS):\n",
    "        v = 0\n",
    "        for a, action_prob in enumerate(policy[s]):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "        delta = max(delta, np.abs(v - V[s]))\n",
    "        V[s] = v\n",
    "    \n",
    "\n",
    "    # raise NotImplementedError\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.       , -1.       , -1.25     , -1.3125   , -1.       ,\n",
       "       -1.5      , -1.6875   , -1.75     , -1.25     , -1.6875   ,\n",
       "       -1.84375  , -1.8984375, -1.3125   , -1.75     , -1.8984375,\n",
       "        0.       ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run your code, does it make sense?\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "V = policy_eval_v(random_policy, env)\n",
    "assert V.shape == (env.nS)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAGiCAYAAADeGX1SAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7G0lEQVR4nO3de3QU9f3/8dcacgEkixCTLIdAQqUBuWhIqgRvaGwQFG8UpFLUfmlaLBcx5aCh3yrQr6ZaWvFSQWwQENpw2pAWD8iXoElQIS2BRC5iytcCCTQxYmEDVJIQ5/cHZH8uuc5MslnI83HOnMPMfmbmM8yRffn+fGbWYRiGIQAAAB+6oqM7AAAAOh8CCAAA8DkCCAAA8DkCCAAA8DkCCAAA8DkCCAAA8DkCCAAA8DkCCAAA8DkCCAAA8DkCCAAA8DlLAeT1119XTEyMQkJCFB8frw8++KDZ9vn5+YqPj1dISIgGDBigZcuWWeosAABon+/hrKwsXXvttQoODta1116r7Ozs9ur+eYZJmZmZRmBgoPHmm28an3zyifHEE08Y3bt3N44cOdJo+3/+859Gt27djCeeeML45JNPjDfffNMIDAw0/vznP5s9NQAAnV57fA9v377dCAgIMJ5//nnjwIEDxvPPP2906dLFKCgoaLfrcBiGuR+ju/HGGzVixAgtXbrUs23w4MG6//77lZ6e3qD9U089pQ0bNujAgQOebdOnT9fHH3+sHTt22IhOAAB0Pu3xPfzQQw+pqqpK7777rqfNXXfdpauuukp//OMf2+U6uphpXFNTo127dunpp5/22p6cnKzt27c3us+OHTuUnJzstW3MmDHKyMhQbW2tAgMDG+xTXV2t6upqz/rXX3+tf//73+rdu7ccDoeZLgMAOhnDMHTq1Cn16dNHV1zRPlMdz549q5qamjY5lmEYDb7bgoODFRwc3KBte30P79ixQ08++WSDNkuWLLFwRa1jKoAcP35cdXV1ioiI8NoeERGhioqKRvepqKhotP25c+d0/PhxuVyuBvukp6dr4cKFZroGAICXsrIy9e3bt82Pe/bsWcXExDT5vWfWlVdeqdOnT3tte/bZZ7VgwYIGbdvre7ipNm11jY0xFUDqXZzUGktvLbVvbHu9tLQ0paametbdbrf69eunsrIyhYaGWuky2hDh0H8cPny4o7uAC86ePdvRXcAFtbW1ysnJUY8ePdrl+DU1NaqoqFBpaant76SqqqpGv98aq358U3t8D5s9pl2mAkhYWJgCAgIaJKLKysoGyaleZGRko+27dOmi3r17N7pPU6Wn0NBQAogfaOk/DPhOY0OY6Bh1dXUd3QVcpL2H7NvyO6m1x2qv7+Gm2jR1zLZganAsKChI8fHxysnJ8dqek5OjUaNGNbpPYmJig/ZbtmxRQkIC/3gCAC5ZhmG0yWJGe30PN9WmqWO2BdOzc1JTU/X73/9eK1as0IEDB/Tkk0+qtLRU06dPl3R++OSRRx7xtJ8+fbqOHDmi1NRUHThwQCtWrFBGRobmzp3bdlcBAICPdUQAkdrne/iJJ57Qli1b9MILL+jTTz/VCy+8oK1bt2rOnDm2/56aYnoOyEMPPaQvv/xSixYtUnl5uYYOHapNmzapf//+kqTy8nKVlpZ62sfExGjTpk168skn9bvf/U59+vTRK6+8ogkTJrTdVQAA4GNWA8TFxzCrPb6HR40apczMTP33f/+3fvGLX+hb3/qW1q1bpxtvvNHW9TXH9HtAOkJVVZWcTqfcbjdzQPzA/PnzO7oLuOCf//xnR3cBFzAJ1X/U1tZq06ZN7fadUf+d9OWXX7bJJNTevXt3yu83S0/BAADQ2XVUBeRyQQABAMACAog9/BouAADwOSogAABYQAXEHgIIAAAWEEDsYQgGAAD4HBUQAAAsoAJiDwEEAAALCCD2MAQDAAB8jgoIAAAWUAGxhwACAIAFBBB7CCAAAFhAALGHOSAAAMDnqIAAAGABFRB7CCAAAFhAALGHIRgAAOBzVEAAALCACog9BBAAACwggNjDEAwAAPA5KiAAAFhABcQeAggAABZ15gBhF0MwAADA56iAAABgAUMw9hBAAACwgABiDwEEAAALCCD2MAcEAAD4HBUQAAAsoAJiDwEEAAALCCD2MAQDAAB8jgoIAAAWUAGxhwACAIAFBBB7GIIBAAA+RwUEAAALqIDYQwABAMACAog9DMEAAACfsxRAXn/9dcXExCgkJETx8fH64IMPmmybl5cnh8PRYPn0008tdxoAgI5WXwGxu3RWpodg1q1bpzlz5uj111/XTTfdpDfeeENjx47VJ598on79+jW5X0lJiUJDQz3rV199tbUeAwDgBxiCscd0BeS3v/2tpk2bph/96EcaPHiwlixZoqioKC1durTZ/cLDwxUZGelZAgICLHcaAICORgXEHlMBpKamRrt27VJycrLX9uTkZG3fvr3ZfePi4uRyuZSUlKTc3Nxm21ZXV6uqqsprAQAArXfixAlNnTpVTqdTTqdTU6dO1cmTJ5tsX1tbq6eeekrDhg1T9+7d1adPHz3yyCP617/+5dVu9OjRDaZVTJ482XT/TAWQ48ePq66uThEREV7bIyIiVFFR0eg+LpdLy5cvV1ZWltavX6/Y2FglJSVp27ZtTZ4nPT3d8xfmdDoVFRVlppsAALQ7f6+APPzwwyouLtbmzZu1efNmFRcXa+rUqU22/89//qPdu3frF7/4hXbv3q3169frH//4h+69994GbVNSUlReXu5Z3njjDdP9s/QYrsPh8Fo3DKPBtnqxsbGKjY31rCcmJqqsrEyLFy/Wrbfe2ug+aWlpSk1N9axXVVURQgAAfqUt54BcXOkPDg5WcHCw5eMeOHBAmzdvVkFBgW688UZJ0ptvvqnExESVlJR4fS/XczqdysnJ8dr26quv6oYbblBpaanXPM9u3bopMjLScv8kkxWQsLAwBQQENKh2VFZWNqiKNGfkyJE6ePBgk58HBwcrNDTUawEA4HIVFRXlVflPT0+3dbwdO3bI6XR6wod0/rvX6XS2OGXim9xutxwOh3r27Om1fe3atQoLC9OQIUM0d+5cnTp1ynQfTVVAgoKCFB8fr5ycHD3wwAOe7Tk5ObrvvvtafZyioiK5XC4zpwYAwK+0ZQWkrKzM63+27VQ/JKmiokLh4eENtoeHhzc5ZeJiZ8+e1dNPP62HH37Yq29TpkxRTEyMIiMjtW/fPqWlpenjjz9uUD1piekhmNTUVE2dOlUJCQlKTEzU8uXLVVpaqunTp0s6P3xy7NgxrV69WpK0ZMkSRUdHa8iQIaqpqdGaNWuUlZWlrKwss6cGAMBvtGUAaW21f8GCBVq4cGGzbXbu3Cmp4XSJ+vM1NWXim2prazV58mR9/fXXev31170+S0lJ8fx56NChGjhwoBISErR7926NGDGixWPXMx1AHnroIX355ZdatGiRysvLNXToUG3atEn9+/eXJJWXl6u0tNTTvqamRnPnztWxY8fUtWtXDRkyRBs3btS4cePMnhoAgE5t5syZLT5xEh0drT179ujzzz9v8NkXX3zR4pSJ2tpaTZo0SYcOHdL777/fYjAaMWKEAgMDdfDgwfYNIJL005/+VD/96U8b/WzlypVe6/PmzdO8efOsnAYAAL/VES8iCwsLU1hYWIvtEhMT5Xa79fe//1033HCDJOlvf/ub3G63Ro0a1eR+9eHj4MGDys3NVe/evVs81/79+1VbW2t6agW/BQMAgEX++gju4MGDdddddyklJUUFBQUqKChQSkqK7rnnHq8nYAYNGqTs7GxJ0rlz5/S9731PhYWFWrt2rerq6lRRUaGKigrV1NRIkj777DMtWrRIhYWFOnz4sDZt2qSJEycqLi5ON910k6k+EkAAALgMrV27VsOGDVNycrKSk5M1fPhwvf32215tSkpK5Ha7JUlHjx7Vhg0bdPToUV1//fVyuVyepf7JmaCgIL333nsaM2aMYmNjNXv2bCUnJ2vr1q2m33BuaQgGAIDOzt9/C6ZXr15as2ZNq88fHR3dYn+ioqKUn5/fJv0jgAAAYIG/BxB/RwABAMACAog9zAEBAAA+RwUEAAALqIDYQwABAMACAog9DMEAAACfowICAIAFVEDsIYAAAGABAcQehmAAAIDPUQEBAMACKiD2EEAAALCAAGIPQzAAAMDnqIAAAGABFRB7CCAAAFhAALGHAAIAgAUEEHuYAwIAAHyOCggAABZQAbGHAAIAgAUEEHsYggEAAD5HBQQAAAuogNhDAAEAwAICiD0MwQAAAJ+jAgIAgAVUQOwhgAAAYFFnDhB2MQQDAAB8jgoIAAAWMARjDwEEAAALCCD2EEAAALCAAGIPc0AAAIDPUQEBAMACKiD2EEAAALCAAGIPQzAAAMDnTAeQbdu2afz48erTp48cDof+8pe/tLhPfn6+4uPjFRISogEDBmjZsmVW+goAgN+or4DYXTor0wHkzJkzuu666/Taa6+1qv2hQ4c0btw43XLLLSoqKtL8+fM1e/ZsZWVlme4sAAD+ggBij+k5IGPHjtXYsWNb3X7ZsmXq16+flixZIkkaPHiwCgsLtXjxYk2YMMHs6QEAwGWg3eeA7NixQ8nJyV7bxowZo8LCQtXW1ja6T3V1taqqqrwWAAD8CRUQe9r9KZiKigpFRER4bYuIiNC5c+d0/PhxuVyuBvukp6dr4cKFDbYvXLhQwcHB7dZXtM7WrVs7ugu4oLy8vKO7gAvOnj3b0V3ABV9//bVPzsNTMPb45CkYh8PhtV7/F37x9nppaWlyu92epaysrN37CADA5eTEiROaOnWqnE6nnE6npk6dqpMnTza7z2OPPSaHw+G1jBw50qtNdXW1Zs2apbCwMHXv3l333nuvjh49arp/7R5AIiMjVVFR4bWtsrJSXbp0Ue/evRvdJzg4WKGhoV4LAAD+xN+HYB5++GEVFxdr8+bN2rx5s4qLizV16tQW97vrrrtUXl7uWTZt2uT1+Zw5c5Sdna3MzEx9+OGHOn36tO655x7V1dWZ6l+7D8EkJibqnXfe8dq2ZcsWJSQkKDAwsL1PDwBAu/DnIZgDBw5o8+bNKigo0I033ihJevPNN5WYmKiSkhLFxsY2uW9wcLAiIyMb/cztdisjI0Nvv/227rzzTknSmjVrFBUVpa1bt2rMmDGt7qPpCsjp06dVXFys4uJiSecfsy0uLlZpaamk88MnjzzyiKf99OnTdeTIEaWmpurAgQNasWKFMjIyNHfuXLOnBgDAb7RlBeTiBy+qq6tt9W3Hjh1yOp2e8CFJI0eOlNPp1Pbt25vdNy8vT+Hh4fr2t7+tlJQUVVZWej7btWuXamtrvR4u6dOnj4YOHdricS9mOoAUFhYqLi5OcXFxkqTU1FTFxcXpmWeekXR+Ulx9GJGkmJgYbdq0SXl5ebr++uv1y1/+Uq+88gqP4AIAcEFUVJRnrobT6VR6erqt41VUVCg8PLzB9vDw8AbTIr5p7NixWrt2rd5//3395je/0c6dO3XHHXd4AlFFRYWCgoJ01VVXee0XERHR7HEbY3oIZvTo0c2WjFauXNlg22233abdu3ebPRUAAH6rLYdgysrKvOY7NvXE54IFCxp9SvSbdu7cKanxBz0Mw2jyARBJeuihhzx/Hjp0qBISEtS/f39t3LhRDz74YLPX0dxxG8OP0QEAYEFbBpDWPnAxc+ZMTZ48udk20dHR2rNnjz7//PMGn33xxRcNXo3RHJfLpf79++vgwYOSzj9YUlNToxMnTnhVQSorKzVq1KhWH1cigAAAcMkICwtTWFhYi+0SExPldrv197//XTfccIMk6W9/+5vcbrepoPDll1+qrKzM886u+Ph4BQYGKicnR5MmTZJ0furFvn379OKLL5q6Fn4NFwAAC/z5MdzBgwfrrrvuUkpKigoKClRQUKCUlBTdc889Xk/ADBo0SNnZ2ZLOP2Qyd+5c7dixQ4cPH1ZeXp7Gjx+vsLAwPfDAA5Ikp9OpadOm6Wc/+5nee+89FRUV6Qc/+IGGDRvmeSqmtaiAAABggT8/hitJa9eu1ezZsz1PrNx7770Nfki2pKREbrdbkhQQEKC9e/dq9erVOnnypFwul26//XatW7dOPXr08Ozz0ksvqUuXLpo0aZK++uorJSUlaeXKlQoICDDVPwIIAACXoV69emnNmjXNtvlmAOratav+93//t8XjhoSE6NVXX9Wrr75qq38EEAAALPD3Coi/I4AAAGBRZw4QdjEJFQAA+BwVEAAALGAIxh4CCAAAFhBA7CGAAABgAQHEHuaAAAAAn6MCAgCABVRA7CGAAABgAQHEHoZgAACAz1EBAQDAAiog9hBAAACwgABiD0MwAADA56iAAABgARUQewggAABYQACxhyEYAADgc1RAAACwgAqIPQQQAAAsIIDYQwABAMACAog9zAEBAAA+RwUEAAALqIDYQwABAMACAog9DMEAAACfowICAIAFVEDsIYAAAGABAcQehmAAAIDPUQEBAMACKiD2EEAAALCAAGIPQzAAAMDnqIAAAGBRZ65g2EUAAQDAAoZg7DE9BLNt2zaNHz9effr0kcPh0F/+8pdm2+fl5cnhcDRYPv30U6t9BgCgw9UHELtLZ2W6AnLmzBldd911+uEPf6gJEya0er+SkhKFhoZ61q+++mqzpwYAAJcJ0wFk7NixGjt2rOkThYeHq2fPnq1qW11drerqas96VVWV6fMBANCeGIKxx2dPwcTFxcnlcikpKUm5ubnNtk1PT5fT6fQsUVFRPuolAACtwxCMPe0eQFwul5YvX66srCytX79esbGxSkpK0rZt25rcJy0tTW6327OUlZW1dzcBAIAPtftTMLGxsYqNjfWsJyYmqqysTIsXL9att97a6D7BwcEKDg5u764BAGAZQzD2dMiLyEaOHKmDBw92xKkBAGgTDMHY0yEBpKioSC6XqyNODQBAp3DixAlNnTrVM59y6tSpOnnyZLP7NPbaDIfDoV//+teeNqNHj27w+eTJk033z/QQzOnTp/V///d/nvVDhw6puLhYvXr1Ur9+/ZSWlqZjx45p9erVkqQlS5YoOjpaQ4YMUU1NjdasWaOsrCxlZWWZ7iwAAP7C34dgHn74YR09elSbN2+WJP34xz/W1KlT9c477zS5T3l5udf6u+++q2nTpjV47UZKSooWLVrkWe/atavp/pkOIIWFhbr99ts966mpqZKkRx99VCtXrlR5eblKS0s9n9fU1Gju3Lk6duyYunbtqiFDhmjjxo0aN26c6c4CAOAv/DmAHDhwQJs3b1ZBQYFuvPFGSdKbb76pxMRElZSUeM3N/KbIyEiv9b/+9a+6/fbbNWDAAK/t3bp1a9DWLNMBZPTo0c3+ha1cudJrfd68eZo3b57pjgEA4M/aMoBc/L4ruw9j7NixQ06n0xM+pPPzL51Op7Zv395kAPmmzz//XBs3btSqVasafLZ27VqtWbNGERERGjt2rJ599ln16NHDVB/5LRgAADrYxe+7evbZZ7VgwQLLx6uoqFB4eHiD7eHh4aqoqGjVMVatWqUePXrowQcf9No+ZcoUxcTEKDIyUvv27VNaWpo+/vhj5eTkmOojAQQAAAvasgJSVlbm9XMlTVU/FixYoIULFzZ7zJ07d0o6P6G0sfM1tr0xK1as0JQpUxQSEuK1PSUlxfPnoUOHauDAgUpISNDu3bs1YsSIVh1bIoAAAGBJWwaQ0NBQrwDSlJkzZ7b4xEl0dLT27Nmjzz//vMFnX3zxhSIiIlo8zwcffKCSkhKtW7euxbYjRoxQYGCgDh48SAABAOByFBYWprCwsBbbJSYmyu126+9//7tuuOEGSdLf/vY3ud1ujRo1qsX9MzIyFB8fr+uuu67Ftvv371dtba3p12t0yHtAAAC41Pnzi8gGDx6su+66SykpKSooKFBBQYFSUlJ0zz33eE1AHTRokLKzs732raqq0p/+9Cf96Ec/anDczz77TIsWLVJhYaEOHz6sTZs2aeLEiYqLi9NNN91kqo8EEAAALPDnACKdf1Jl2LBhSk5OVnJysoYPH663337bq01JSYncbrfXtszMTBmGoe9///sNjhkUFKT33ntPY8aMUWxsrGbPnq3k5GRt3bpVAQEBpvrHEAwAAJehXr16ac2aNc22aSwA/fjHP9aPf/zjRttHRUUpPz+/TfpHAAEAwAJ/fhHZpYAAAgCABQQQe5gDAgAAfI4KCAAAFlABsYcAAgCABQQQewggAABY1JkDhF3MAQEAAD5HBQQAAAsYgrGHAAIAgAUEEHsYggEAAD5HBQQAAAuogNhDAAEAwAICiD0MwQAAAJ+jAgIAgAVUQOwhgAAAYAEBxB6GYAAAgM9RAQEAwAIqIPYQQAAAsIAAYg8BBAAACwgg9jAHBAAA+BwVEAAALKACYg8BBAAACwgg9jAEAwAAfI4KCAAAFlABsYcAAgCABQQQexiCAQAAPkcFBAAAC6iA2EMAAQDAAgKIPaaGYNLT0/Wd73xHPXr0UHh4uO6//36VlJS0uF9+fr7i4+MVEhKiAQMGaNmyZZY7DAAALn2mAkh+fr5mzJihgoIC5eTk6Ny5c0pOTtaZM2ea3OfQoUMaN26cbrnlFhUVFWn+/PmaPXu2srKybHceAICOUl8Bsbt0VqaGYDZv3uy1/tZbbyk8PFy7du3Srbfe2ug+y5YtU79+/bRkyRJJ0uDBg1VYWKjFixdrwoQJje5TXV2t6upqz3pVVZWZbgIA0O4YgrHH1hwQt9stSerVq1eTbXbs2KHk5GSvbWPGjFFGRoZqa2sVGBjYYJ/09HQtXLiwwfbDhw832h6+VV5e3tFdwAVffPFFR3cBF3zzf5rQeXTmAGGX5cdwDcNQamqqbr75Zg0dOrTJdhUVFYqIiPDaFhERoXPnzun48eON7pOWlia32+1ZysrKrHYTAAD4IcsVkJkzZ2rPnj368MMPW2zrcDi81usT48Xb6wUHBys4ONhq1wAAaHcMwdhjKYDMmjVLGzZs0LZt29S3b99m20ZGRqqiosJrW2Vlpbp06aLevXtbOT0AAB2OAGKPqSEYwzA0c+ZMrV+/Xu+//75iYmJa3CcxMVE5OTle27Zs2aKEhATmcwAA0EmZCiAzZszQmjVr9Ic//EE9evRQRUWFKioq9NVXX3napKWl6ZFHHvGsT58+XUeOHFFqaqoOHDigFStWKCMjQ3Pnzm27qwAAwMd4DNceUwFk6dKlcrvdGj16tFwul2dZt26dp015eblKS0s96zExMdq0aZPy8vJ0/fXX65e//KVeeeWVJh/BBQDgUkAAscf0EExjy2OPPeZps3LlSuXl5Xntd9ttt2n37t2qrq7WoUOHNH369LboOwAAaMJzzz2nUaNGqVu3burZs2er9jEMQwsWLFCfPn3UtWtXjR49Wvv37/dqU11drVmzZiksLEzdu3fXvffeq6NHj5ruH7+GCwCABf5eAampqdHEiRP1+OOPt3qfF198Ub/97W/12muvaefOnYqMjNR3v/tdnTp1ytNmzpw5ys7OVmZmpj788EOdPn1a99xzj+rq6kz1jx+jAwDAgrZ8CubiN363xeso6l/ouXLlylb3ZcmSJfr5z3+uBx98UJK0atUqRURE6A9/+IN+8pOfyO12KyMjQ2+//bbuvPNOSdKaNWsUFRWlrVu3asyYMa3uHxUQAAA6WFRUlJxOp2dJT0/3eR8OHTqkiooKr7eXBwcH67bbbtP27dslSbt27VJtba1Xmz59+mjo0KGeNq1FBQQAAAvasgJSVlam0NBQz/aOeBln/Tu7Gnt7+ZEjRzxtgoKCdNVVVzVoc/E7v1pCBQQAAAvacg5IaGio19JUAFmwYIEcDkezS2Fhoa3rauzt5U29udxMm4tRAQEAwIKOeBPqzJkzNXny5GbbREdHW+pLZGSkpPNVDpfL5dleWVnpqYpERkaqpqZGJ06c8KqCVFZWatSoUabORwABAOASERYWprCwsHY5dkxMjCIjI5WTk6O4uDhJ55+kyc/P1wsvvCBJio+PV2BgoHJycjRp0iRJ59//tW/fPr344oumzkcAAQDAAn//LZjS0lL9+9//Vmlpqerq6lRcXCxJuuaaa3TllVdKkgYNGqT09HQ98MADcjgcmjNnjp5//nkNHDhQAwcO1PPPP69u3brp4YcfliQ5nU5NmzZNP/vZz9S7d2/16tVLc+fO1bBhwzxPxbQWAQQAAAv8PYA888wzWrVqlWe9vqqRm5ur0aNHS5JKSkrkdrs9bebNm6evvvpKP/3pT3XixAndeOON2rJli3r06OFp89JLL6lLly6aNGmSvvrqKyUlJWnlypUKCAgw1T+HcQm8B7aqqkpOp1MPPvggP2DnBz766KOO7gIu+OKLLzq6C7igurq6o7uAi7jdbq8nS9pK/XfSrFmzbD+tUl1drVdffbXd+urPqIAAAGCBv1dA/B0BBAAACwgg9vAeEAAA4HNUQAAAsIAKiD0EEAAALCCA2MMQDAAA8DkqIAAAWEAFxB4CCAAAFhBA7CGAAABgUWcOEHYxBwQAAPgcFRAAACxgCMYeAggAABYQQOxhCAYAAPgcFRAAACygAmIPAQQAAAsIIPYwBAMAAHyOCggAABZQAbGHAAIAgAUEEHsYggEAAD5HBQQAAAuogNhDAAEAwAICiD0EEAAALCCA2MMcEAAA4HNUQAAAsIAKiD0EEAAALCCA2GNqCCY9PV3f+c531KNHD4WHh+v+++9XSUlJs/vk5eXJ4XA0WD799FNbHQcAAJcuUwEkPz9fM2bMUEFBgXJycnTu3DklJyfrzJkzLe5bUlKi8vJyzzJw4EDLnQYAoKPVV0DsLp2VqSGYzZs3e62/9dZbCg8P165du3Trrbc2u294eLh69uxpuoMAAPgjhmDssfUUjNvtliT16tWrxbZxcXFyuVxKSkpSbm5us22rq6tVVVXltQAAgMuH5QBiGIZSU1N18803a+jQoU22c7lcWr58ubKysrR+/XrFxsYqKSlJ27Zta3Kf9PR0OZ1OzxIVFWW1mwAAtAuGYOyx/BTMzJkztWfPHn344YfNtouNjVVsbKxnPTExUWVlZVq8eHGTwzZpaWlKTU31rFdVVRFCAAB+hSEYeyxVQGbNmqUNGzYoNzdXffv2Nb3/yJEjdfDgwSY/Dw4OVmhoqNcCAAAuH6YqIIZhaNasWcrOzlZeXp5iYmIsnbSoqEgul8vSvgAA+AMqIPaYCiAzZszQH/7wB/31r39Vjx49VFFRIUlyOp3q2rWrpPPDJ8eOHdPq1aslSUuWLFF0dLSGDBmimpoarVmzRllZWcrKymrjSwEAwHcIIPaYCiBLly6VJI0ePdpr+1tvvaXHHntMklReXq7S0lLPZzU1NZo7d66OHTumrl27asiQIdq4caPGjRtnr+cAAHSwzhwg7DI9BNOSlStXeq3PmzdP8+bNM9UpAABweeO3YAAAsIAhGHtsvYgMAIDOyt/fA/Lcc89p1KhR6tatW6veRF5bW6unnnpKw4YNU/fu3dWnTx898sgj+te//uXVbvTo0Q1+323y5Mmm+0cAAQDgMlRTU6OJEyfq8ccfb1X7//znP9q9e7d+8YtfaPfu3Vq/fr3+8Y9/6N57723QNiUlxev33d544w3T/WMIBgAAC/x9CGbhwoWSGs7NbIrT6VROTo7XtldffVU33HCDSktL1a9fP8/2bt26KTIy0lb/qIAAAGBBWw7BXPz7Z9XV1R18dee53W45HI4GQzhr165VWFiYhgwZorlz5+rUqVOmj00FBACADnbxz408++yzWrBgQcd05oKzZ8/q6aef1sMPP+z1RvIpU6YoJiZGkZGR2rdvn9LS0vTxxx83qJ60hAACAIAFbTkEU1ZW5vUlHxwc3Gj7BQsWeIZWmrJz504lJCTY6ldtba0mT56sr7/+Wq+//rrXZykpKZ4/Dx06VAMHDlRCQoJ2796tESNGtPocBBAAACxoywDS2t89mzlzZotPnERHR9vqU21trSZNmqRDhw7p/fffb7FfI0aMUGBgoA4ePEgAAQDgchQWFqawsLB2O359+Dh48KByc3PVu3fvFvfZv3+/amtrTf/GG5NQAQCwwN/fA1JaWqri4mKVlpaqrq5OxcXFKi4u1unTpz1tBg0apOzsbEnSuXPn9L3vfU+FhYVau3at6urqVFFRoYqKCtXU1EiSPvvsMy1atEiFhYU6fPiwNm3apIkTJyouLk433XSTqf5RAQEAwAJ/fwz3mWee0apVqzzrcXFxkqTc3FzPb7qVlJTI7XZLko4ePaoNGzZIkq6//nqvY9XvExQUpPfee08vv/yyTp8+raioKN1999169tlnFRAQYKp/BBAAACzw9wCycuXKFt8B8s3zR0dHt9ifqKgo5efnt0X3GIIBAAC+RwUEAAAL/L0C4u8IIAAAWEAAsYchGAAA4HNUQAAAsIAKiD0EEAAALCCA2MMQDAAA8DkqIAAAWEAFxB4CCAAAFhBA7GEIBgAA+BwVEAAALKACYg8BBAAACwgg9hBAAACwgABiD3NAAACAz1EBAQDAos5cwbCLAAIAgAUMwdjDEAwAAPA5KiAAAFhABcQeAggAABYQQOxhCAYAAPgcFRAAACygAmIPAQQAAAsIIPYwBAMAAHzOVABZunSphg8frtDQUIWGhioxMVHvvvtus/vk5+crPj5eISEhGjBggJYtW2arwwAA+IP6CojdpbMyFUD69u2rX/3qVyosLFRhYaHuuOMO3Xfffdq/f3+j7Q8dOqRx48bplltuUVFRkebPn6/Zs2crKyurTToPAEBHIYDYY2oOyPjx473Wn3vuOS1dulQFBQUaMmRIg/bLli1Tv379tGTJEknS4MGDVVhYqMWLF2vChAnWew0AQAdjDog9lueA1NXVKTMzU2fOnFFiYmKjbXbs2KHk5GSvbWPGjFFhYaFqa2ubPHZ1dbWqqqq8FgAAcPkw/RTM3r17lZiYqLNnz+rKK69Udna2rr322kbbVlRUKCIiwmtbRESEzp07p+PHj8vlcjW6X3p6uhYuXNhg+9mzZ1VXV2e2y2hjZ8+e7egu4ILq6uqO7gIu6Mz/J+tvqqqq5HQ62/08VEDsMV0BiY2NVXFxsQoKCvT444/r0Ucf1SeffNJke4fD4bVe/5d98fZvSktLk9vt9ixlZWVmuwkAQLtiDog9pisgQUFBuuaaayRJCQkJ2rlzp15++WW98cYbDdpGRkaqoqLCa1tlZaW6dOmi3r17N3mO4OBgBQcHm+0aAAC4RNh+EZlhGE2WgRMTE/XOO+94bduyZYsSEhIUGBho99QAAHQYhmDsMTUEM3/+fH3wwQc6fPiw9u7dq5///OfKy8vTlClTJJ0fOnnkkUc87adPn64jR44oNTVVBw4c0IoVK5SRkaG5c+e27VUAAOBjDMHYY6oC8vnnn2vq1KkqLy+X0+nU8OHDtXnzZn33u9+VJJWXl6u0tNTTPiYmRps2bdKTTz6p3/3ud+rTp49eeeUVHsEFAKCTMxVAMjIymv185cqVDbbddttt2r17t6lOAQDg7xiCsYcfowMAwAICiD38GB0AAPA5KiAAAFhABcQeAggAABYQQOxhCAYAAAv8/THc5557TqNGjVK3bt3Us2fPVu3z2GOPyeFweC0jR470alNdXa1Zs2YpLCxM3bt317333qujR4+a7h8BBACAy1BNTY0mTpyoxx9/3NR+d911l8rLyz3Lpk2bvD6fM2eOsrOzlZmZqQ8//FCnT5/WPffcY/q32hiCAQDAIn8eQqn/UdfGXpHRnODgYEVGRjb6mdvtVkZGht5++23deeedkqQ1a9YoKipKW7du1ZgxY1p9HiogAABY0JZDMFVVVV5LR/7SdV5ensLDw/Xtb39bKSkpqqys9Hy2a9cu1dbWKjk52bOtT58+Gjp0qLZv327qPAQQAAA6WFRUlJxOp2dJT0/vkH6MHTtWa9eu1fvvv6/f/OY32rlzp+644w5PIKqoqFBQUJCuuuoqr/0iIiIa/PhsSxiCAQDAgrZ8CqasrEyhoaGe7U39IvyCBQs8QytN2blzpxISEiz156GHHvL8eejQoUpISFD//v21ceNGPfjgg03uZxiGHA6HqXMRQAAAsKAtA0hoaKhXAGnKzJkzNXny5GbbREdH2+rTN7lcLvXv318HDx6UJEVGRqqmpkYnTpzwqoJUVlZq1KhRpo5NAAEA4BIRFhamsLAwn53vyy+/VFlZmVwulyQpPj5egYGBysnJ0aRJkySd/yHaffv26cUXXzR1bOaAAABggb+/B6S0tFTFxcUqLS1VXV2diouLVVxcrNOnT3vaDBo0SNnZ2ZKk06dPa+7cudqxY4cOHz6svLw8jR8/XmFhYXrggQckSU6nU9OmTdPPfvYzvffeeyoqKtIPfvADDRs2zPNUTGtRAQEAwAJ/fxPqM888o1WrVnnW4+LiJEm5ubkaPXq0JKmkpERut1uSFBAQoL1792r16tU6efKkXC6Xbr/9dq1bt049evTwHOell15Sly5dNGnSJH311VdKSkrSypUrFRAQYKp/DsOfH2K+oKqqSk6nU+PGjVNgYGBHd6fT++ijjzq6C7jg+PHjHd0FXHAJ/FPaadR/Z7jd7lbNq7B6/OHDh5v+0r1YXV2d9uzZ02599WdUQAAAsMDfKyD+jgACAIAFBBB7CCAAAFhAALGHp2AAAIDPUQEBAMACKiD2EEAAALCAAGIPQzAAAMDnqIAAAGABFRB7CCAAAFhAALGHIRgAAOBzVEAAALCACog9BBAAACwggNjDEAwAAPA5KiAAAFhABcQeAggAABYQQOwhgAAAYAEBxB7mgAAAAJ+jAgIAgEWduYJhFwEEAAALGIKxhyEYAADgc6YCyNKlSzV8+HCFhoYqNDRUiYmJevfdd5tsn5eXJ4fD0WD59NNPbXccAICOVF8Bsbt0VqaGYPr27atf/epXuuaaayRJq1at0n333aeioiINGTKkyf1KSkoUGhrqWb/66qstdhcAAP/AEIw9pgLI+PHjvdafe+45LV26VAUFBc0GkPDwcPXs2dNSBwEAwOXH8hyQuro6ZWZm6syZM0pMTGy2bVxcnFwul5KSkpSbm9visaurq1VVVeW1AADgTxiCscd0ANm7d6+uvPJKBQcHa/r06crOzta1117baFuXy6Xly5crKytL69evV2xsrJKSkrRt27Zmz5Geni6n0+lZoqKizHYTAIB2RQCxx2GYvPqamhqVlpbq5MmTysrK0u9//3vl5+c3GUIuNn78eDkcDm3YsKHJNtXV1aqurvasV1VVKSoqSuPGjVNgYKCZ7qIdfPTRRx3dBVxw/Pjxju4CLujMXyT+pqqqSk6nU26322v+YVsfPyoqSldcYe9h0q+//lplZWXt1ld/Zvo9IEFBQZ5JqAkJCdq5c6defvllvfHGG63af+TIkVqzZk2zbYKDgxUcHGy2awAA+AyTUO2x/SIywzC8qhUtKSoqksvlsntaAAA6FAHEHlMBZP78+Ro7dqyioqJ06tQpZWZmKi8vT5s3b5YkpaWl6dixY1q9erUkacmSJYqOjtaQIUNUU1OjNWvWKCsrS1lZWW1/JQAA+BABxB5TAeTzzz/X1KlTVV5eLqfTqeHDh2vz5s367ne/K0kqLy9XaWmpp31NTY3mzp2rY8eOqWvXrhoyZIg2btyocePGte1VAACAS4rpSagdoX7CD5NQ/QOTUP0Hk1D9xyXwT2mn4atJqC6Xq00moZaXlzMJFQAAtA5DMPbwY3QAAMDnqIAAAGABFRB7CCAAAFhAALGHIRgAAOBzVEAAALCACog9BBAAACwggNjDEAwAAJeh5557TqNGjVK3bt3Us2fPVu3jcDgaXX7961972owePbrB55MnTzbdPyogAABY4O8VkJqaGk2cOFGJiYnKyMho1T7l5eVe6++++66mTZumCRMmeG1PSUnRokWLPOtdu3Y13T8CCAAAFrRlAKmqqvLa3ha/Cr9w4UJJ0sqVK1u9T2RkpNf6X//6V91+++0aMGCA1/Zu3bo1aGsWQzAAAFhQH0DsLpIUFRUlp9PpWdLT0zv46s7//tvGjRs1bdq0Bp+tXbtWYWFhGjJkiObOnatTp06ZPj4VEAAAOlhZWZnXb8HYrX60hVWrVqlHjx568MEHvbZPmTJFMTExioyM1L59+5SWlqaPP/5YOTk5po5PAAEAwKK2msMRGhraqh+jW7BggWdopSk7d+5UQkKC7T6tWLFCU6ZMUUhIiNf2lJQUz5+HDh2qgQMHKiEhQbt379aIESNafXwCCAAAFrRF+DB7jJkzZ7b4xEl0dLSNHp33wQcfqKSkROvWrWux7YgRIxQYGKiDBw8SQAAAuByFhYUpLCys3c+TkZGh+Ph4XXfddS223b9/v2pra+VyuUydg0moAABY0JaTUNtDaWmpiouLVVpaqrq6OhUXF6u4uFinT5/2tBk0aJCys7O99quqqtKf/vQn/ehHP2pwzM8++0yLFi1SYWGhDh8+rE2bNmnixImKi4vTTTfdZKp/VEAAALCgI4ZgzHjmmWe0atUqz3pcXJwkKTc3V6NHj5YklZSUyO12e+2XmZkpwzD0/e9/v8Exg4KC9N577+nll1/W6dOnFRUVpbvvvlvPPvusAgICTPXPYVwC74GtqqqS0+nUuHHjFBgY2NHd6fQ++uijju4CLjh+/HhHdwEXXAL/lHYa9d8Zbre7VRM7rR4/NDRUDofD1rEMw1BVVVW79dWfUQEBAMACf6+A+DsCCAAAFhBA7GESKgAA8DkqIAAAWEAFxB4CCAAAFhBA7CGAAABgAQHEHuaAAAAAn6MCAgCABVRA7CGAAABgAQHEHoZgAACAz1EBAQDAAiog9hBAAACwgABiD0MwAADA56iAAABgARUQewggAABYQACxhyEYAADgc1RAAACwgAqIPQQQAAAsIIDYY2sIJj09XQ6HQ3PmzGm2XX5+vuLj4xUSEqIBAwZo2bJldk4LAECHMwyjTZbOynIA2blzp5YvX67hw4c32+7QoUMaN26cbrnlFhUVFWn+/PmaPXu2srKyrJ4aAABc4iwNwZw+fVpTpkzRm2++qf/5n/9ptu2yZcvUr18/LVmyRJI0ePBgFRYWavHixZowYUKj+1RXV6u6utqz7na7JUm1tbVWuos29vXXX3d0FwC/U1VV1dFdwAX198IX1YXOXMGwy1IAmTFjhu6++27deeedLQaQHTt2KDk52WvbmDFjlJGRodraWgUGBjbYJz09XQsXLmywPScnx0p3AaDdOZ3Oju4CLvLll1+2y30JCgpSZGSkKioq2uR4kZGRCgoKapNjXUpMB5DMzEzt3r1bO3fubFX7iooKRUREeG2LiIjQuXPndPz4cblcrgb7pKWlKTU11bN+8uRJ9e/fX6WlpZfsf+RVVVWKiopSWVmZQkNDO7o7lnEd/uNyuAbp8riOy+EapMvnOtxut/r166devXq1y/FDQkJ06NAh1dTUtMnxgoKCFBIS0ibHupSYCiBlZWV64okntGXLFlN/WQ6Hw2u9vmR18fZ6wcHBCg4ObrDd6XRe0v9RSFJoaOglfw0S1+FPLodrkC6P67gcrkG6fK7jiiva71VXISEhnTI0tCVTAWTXrl2qrKxUfHy8Z1tdXZ22bdum1157TdXV1QoICPDap7EyVWVlpbp06aLevXvb6DoAALhUmQogSUlJ2rt3r9e2H/7whxo0aJCeeuqpBuFDkhITE/XOO+94bduyZYsSEhIanf8BAAAuf6YCSI8ePTR06FCvbd27d1fv3r0929PS0nTs2DGtXr1akjR9+nS99tprSk1NVUpKinbs2KGMjAz98Y9/bPV5g4OD9eyzzzY6LHOpuByuQeI6/MnlcA3S5XEdl8M1SFwHfMth2HyGaPTo0br++us9j9k+9thjOnz4sPLy8jxt8vPz9eSTT2r//v3q06ePnnrqKU2fPt3OaQEAwCXMdgABAAAwi1/DBQAAPkcAAQAAPkcAAQAAPkcAAQAAPuc3AeT1119XTEyMQkJCFB8frw8++KDZ9vn5+YqPj1dISIgGDBigZcuW+ainTTNzDXl5eXI4HA2WTz/91Ic9bmjbtm0aP368+vTpI4fDob/85S8t7uNv98LsNfjjvUhPT9d3vvMd9ejRQ+Hh4br//vtVUlLS4n7+di+sXIe/3Y+lS5dq+PDhnreDJiYm6t133212H3+7D5L56/C3+9CY9PR0ORwOzZkzp9l2/ng/4CcBZN26dZozZ45+/vOfq6ioSLfccovGjh2r0tLSRtsfOnRI48aN0y233KKioiLNnz9fs2fPVlZWlo97/v+ZvYZ6JSUlKi8v9ywDBw70UY8bd+bMGV133XV67bXXWtXeH++F2Wuo50/3Ij8/XzNmzFBBQYFycnJ07tw5JScn68yZM03u44/3wsp11POX+9G3b1/96le/UmFhoQoLC3XHHXfovvvu0/79+xtt74/3QTJ/HfX85T5cbOfOnVq+fLmGDx/ebDt/vR+QZPiBG264wZg+fbrXtkGDBhlPP/10o+3nzZtnDBo0yGvbT37yE2PkyJHt1seWmL2G3NxcQ5Jx4sQJH/TOGklGdnZ2s2388V58U2uu4VK4F5WVlYYkIz8/v8k2/n4vDKN113Ep3I+rrrrK+P3vf9/oZ5fCfajX3HX48304deqUMXDgQCMnJ8e47bbbjCeeeKLJtpfS/ehsOrwCUlNTo127dik5Odlre3JysrZv397oPjt27GjQfsyYMSosLFRtbW279bUpVq6hXlxcnFwul5KSkpSbm9ue3WwX/nYv7PDne+F2uyWp2V/3vBTuRWuuo54/3o+6ujplZmbqzJkzSkxMbLTNpXAfWnMd9fzxPsyYMUN333237rzzzhbbXgr3o7Pq8ABy/Phx1dXVKSIiwmt7REREgx+xq1dRUdFo+3Pnzun48ePt1temWLkGl8ul5cuXKysrS+vXr1dsbKySkpK0bds2X3S5zfjbvbDC3++FYRhKTU3VzTff3OCnEL7J3+9Fa6/DH+/H3r17deWVVyo4OFjTp09Xdna2rr322kbb+vN9MHMd/ngfJCkzM1O7d+9Wenp6q9r78/3o7Ez9Fkx7cjgcXuuGYTTY1lL7xrb7kplriI2NVWxsrGc9MTFRZWVlWrx4sW699dZ27Wdb88d7YYa/34uZM2dqz549+vDDD1ts68/3orXX4Y/3IzY2VsXFxTp58qSysrL06KOPKj8/v8kvb3+9D2auwx/vQ1lZmZ544glt2bJFISEhrd7PX+9HZ9fhFZCwsDAFBAQ0qBRUVlY2SK31IiMjG23fpUsX9e7du9362hQr19CYkSNH6uDBg23dvXblb/eirfjLvZg1a5Y2bNig3Nxc9e3bt9m2/nwvzFxHYzr6fgQFBemaa65RQkKC0tPTdd111+nll19utK0/3wcz19GYjr4Pu3btUmVlpeLj49WlSxd16dJF+fn5euWVV9SlSxfV1dU12Mef70dn1+EBJCgoSPHx8crJyfHanpOTo1GjRjW6T2JiYoP2W7ZsUUJCggIDA9utr02xcg2NKSoqksvlauvutSt/uxdtpaPvhWEYmjlzptavX6/3339fMTExLe7jj/fCynU0pqPvx8UMw1B1dXWjn/njfWhKc9fRmI6+D0lJSdq7d6+Ki4s9S0JCgqZMmaLi4mIFBAQ02OdSuh+dTodMfb1IZmamERgYaGRkZBiffPKJMWfOHKN79+7G4cOHDcMwjKefftqYOnWqp/0///lPo1u3bsaTTz5pfPLJJ0ZGRoYRGBho/PnPf+6oSzB9DS+99JKRnZ1t/OMf/zD27dtnPP3004YkIysrq6MuwTCM87PLi4qKjKKiIkOS8dvf/tYoKioyjhw5YhjGpXEvzF6DP96Lxx9/3HA6nUZeXp5RXl7uWf7zn/942lwK98LKdfjb/UhLSzO2bdtmHDp0yNizZ48xf/5844orrjC2bNnSaP/98T4Yhvnr8Lf70JSLn4K5VO4HDMMvAohhGMbvfvc7o3///kZQUJAxYsQIr8f0Hn30UeO2227zap+Xl2fExcUZQUFBRnR0tLF06VIf97ghM9fwwgsvGN/61reMkJAQ46qrrjJuvvlmY+PGjR3Qa2/1j95dvDz66KOGYVwa98LsNfjjvWis/5KMt956y9PmUrgXVq7D3+7Hf/3Xf3n+u7766quNpKQkz5e2YVwa98EwzF+Hv92HplwcQC6V+wHDcBjGhdk4AAAAPtLhc0AAAEDnQwABAAA+RwABAAA+RwABAAA+RwABAAA+RwABAAA+RwABAAA+RwABAAA+RwABAAA+RwABAAA+RwABAAA+9/8AnJVWkThpw0cAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_gridworld_value(V):\n",
    "    plt.figure()\n",
    "    c = plt.pcolormesh(V, cmap='gray')\n",
    "    plt.colorbar(c)\n",
    "    plt.gca().invert_yaxis()  # In the array, first row = 0 is on top\n",
    "\n",
    "# Making a plot always helps\n",
    "plot_gridworld_value(V.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Policy Iteration (2 points)\n",
    "Using the policy evaluation algorithm we can implement policy iteration to find a good policy for this problem. Note that we do not need to use a discount_factor for episodic tasks but make sure your implementation can handle this correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def policy_iter_v(env, policy_eval_v=policy_eval_v, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Iteration Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_v: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what it does\n",
    "policy, v = policy_iter_v(env, policy_eval_v)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "def print_grid_policy(policy, symbols=[\"^\", \">\", \"v\", \"<\"]):\n",
    "    symbols = np.array(symbols)\n",
    "    for row in policy:\n",
    "        print(\"\".join(symbols[row]))\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "plot_gridworld_value(v.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Q-value Iteration (3 points)\n",
    "In this exercise you will implement the value iteration algorithm. However, because this algorithm is quite similar to the ones you implemented previously, we will spice things up a bit and use Q-values instead. Thus instead of using Bellman optimality equations for V you will use Bellman equations for Q. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def value_iter_q(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Q-value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all state-action pairs.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, Q) of the optimal policy and the optimal Q-value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with an all 0 Q-value function\n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what it does\n",
    "policy, Q = value_iter_q(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Q Function:\")\n",
    "print(Q)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, the visualization of the Q function is quite clumsy and is not that easy to check \n",
    "# that all values make sense. However, you can easily create a V function from Q and policy to double\n",
    "# check that the values are what you would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dp_autograde.py file into codegrade.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
