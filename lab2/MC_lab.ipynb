{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Reinforcement Learning - Monte Carlo\n","If you want to test/submit your solution **restart the kernel, run all cells and submit the mc_autograde.py file into codegrade.**"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# This cell imports %%execwritefile command (executes cell and writes it into file). \n","# All cells that start with %%execwritefile should be in mc_autograde.py file after running all cells.\n","from custommagics import CustomMagics\n","get_ipython().register_magics(CustomMagics)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting mc_autograde.py\n"]}],"source":["%%execwritefile mc_autograde.py\n","import numpy as np\n","from collections import defaultdict\n","from tqdm import tqdm as _tqdm\n","\n","def tqdm(*args, **kwargs):\n","    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import sys\n","\n","\n","%matplotlib inline\n","\n","assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-7ab207a9f93cf4d3","locked":true,"schema_version":1,"solution":false}},"source":["## 1. Monte Carlo Prediction"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-5f0c1d608436b67b","locked":true,"schema_version":1,"solution":false}},"source":["For the Monte Carlo Prediction we will look at the Blackjack game (Example 5.1 from the book), for which the `BlackjackEnv` is implemented in `blackjack.py`. Note that compared to the gridworld, the state is no longer a single integer, which is why we use a dictionary to represent the value function instead of a numpy array. By using `defaultdict`, each state gets a default value of 0."]},{"cell_type":"code","execution_count":4,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-a342b69fcfdea5b2","locked":true,"schema_version":1,"solution":false}},"outputs":[],"source":["from blackjack import BlackjackEnv\n","env = BlackjackEnv()"]},{"cell_type":"markdown","metadata":{},"source":["For the Monte Carlo algorithm, we no longer have transition probabilities and we need to *interact* with the environment. This means that we start an episode by using `env.reset` and send the environment actions via `env.step` to observe the reward and next observation (state)."]},{"cell_type":"code","execution_count":5,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-85356add2643980e","locked":true,"schema_version":1,"solution":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0;31mInit signature:\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDocstring:\u001b[0m     \n","The main OpenAI Gym class. It encapsulates an environment with\n","arbitrary behind-the-scenes dynamics. An environment can be\n","partially or fully observed.\n","\n","The main API methods that users of this class need to know are:\n","\n","    step\n","    reset\n","    render\n","    close\n","    seed\n","\n","And set the following attributes:\n","\n","    action_space: The Space object corresponding to valid actions\n","    observation_space: The Space object corresponding to valid observations\n","    reward_range: A tuple corresponding to the min and max possible rewards\n","\n","Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.\n","\n","The methods are accessed publicly as \"step\", \"reset\", etc.. The\n","non-underscored versions are wrapper methods to which we may add\n","functionality over time.\n","\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/rlcourse/lib/python3.7/site-packages/gym/core.py\n","\u001b[0;31mType:\u001b[0m           type\n","\u001b[0;31mSubclasses:\u001b[0m     GoalEnv, Wrapper, BlackjackEnv\n"]}],"source":["# So let's have a look at what we can do in general with an environment...\n","import gym\n","?gym.Env"]},{"cell_type":"code","execution_count":6,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-251b7b17c5d08a24","locked":true,"schema_version":1,"solution":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0;31mSignature:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDocstring:\u001b[0m\n","Run one timestep of the environment's dynamics. When end of\n","episode is reached, you are responsible for calling `reset()`\n","to reset this environment's state.\n","\n","Accepts an action and returns a tuple (observation, reward, done, info).\n","\n","Args:\n","    action (object): an action provided by the environment\n","\n","Returns:\n","    observation (object): agent's observation of the current environment\n","    reward (float) : amount of reward returned after previous action\n","    done (boolean): whether the episode has ended, in which case further step() calls will return undefined results\n","    info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n","\u001b[0;31mFile:\u001b[0m      ~/Documents/Master_AI/Year_2/Reinforcement_Learning/Uva_RLcourse_2023/lab2/blackjack.py\n","\u001b[0;31mType:\u001b[0m      method\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[0;31mSignature:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDocstring:\u001b[0m\n","Run one timestep of the environment's dynamics. When end of\n","episode is reached, you are responsible for calling `reset()`\n","to reset this environment's state.\n","\n","Accepts an action and returns a tuple (observation, reward, done, info).\n","\n","Args:\n","    action (object): an action provided by the environment\n","\n","Returns:\n","    observation (object): agent's observation of the current environment\n","    reward (float) : amount of reward returned after previous action\n","    done (boolean): whether the episode has ended, in which case further step() calls will return undefined results\n","    info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n","\u001b[0;31mFile:\u001b[0m      ~/Documents/Master_AI/Year_2/Reinforcement_Learning/Uva_RLcourse_2023/lab2/blackjack.py\n","\u001b[0;31mType:\u001b[0m      method\n"]}],"source":["# We can also look at the documentation/implementation of a method\n","?env.step"]},{"cell_type":"code","execution_count":7,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-6decb2ab83c5bcec","locked":true,"schema_version":1,"solution":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0;31mInit signature:\u001b[0m \u001b[0mBlackjackEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnatural\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mSource:\u001b[0m        \n","\u001b[0;32mclass\u001b[0m \u001b[0mBlackjackEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Simple blackjack environment\u001b[0m\n","\u001b[0;34m    Blackjack is a card game where the goal is to obtain cards that sum to as\u001b[0m\n","\u001b[0;34m    near as possible to 21 without going over.  They're playing against a fixed\u001b[0m\n","\u001b[0;34m    dealer.\u001b[0m\n","\u001b[0;34m    Face cards (Jack, Queen, King) have point value 10.\u001b[0m\n","\u001b[0;34m    Aces can either count as 11 or 1, and it's called 'usable' at 11.\u001b[0m\n","\u001b[0;34m    This game is placed with an infinite deck (or with replacement).\u001b[0m\n","\u001b[0;34m    The game starts with each (player and dealer) having one face up and one\u001b[0m\n","\u001b[0;34m    face down card.\u001b[0m\n","\u001b[0;34m    The player can request additional cards (hit=1) until they decide to stop\u001b[0m\n","\u001b[0;34m    (stick=0) or exceed 21 (bust).\u001b[0m\n","\u001b[0;34m    After the player sticks, the dealer reveals their facedown card, and draws\u001b[0m\n","\u001b[0;34m    until their sum is 17 or greater.  If the dealer goes bust the player wins.\u001b[0m\n","\u001b[0;34m    If neither player nor dealer busts, the outcome (win, lose, draw) is\u001b[0m\n","\u001b[0;34m    decided by whose sum is closer to 21.  The reward for winning is +1,\u001b[0m\n","\u001b[0;34m    drawing is 0, and losing is -1.\u001b[0m\n","\u001b[0;34m    The observation of a 3-tuple of: the players current sum,\u001b[0m\n","\u001b[0;34m    the dealer's one showing card (1-10 where 1 is ace),\u001b[0m\n","\u001b[0;34m    and whether or not the player holds a usable ace (0 or 1).\u001b[0m\n","\u001b[0;34m    This environment corresponds to the version of the blackjack problem\u001b[0m\n","\u001b[0;34m    described in Example 5.1 in Reinforcement Learning: An Introduction\u001b[0m\n","\u001b[0;34m    by Sutton and Barto (1998).\u001b[0m\n","\u001b[0;34m    https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html\u001b[0m\n","\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnatural\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;31m# Flag to payout 1.5 on a \"natural\" blackjack win, like casino rules\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;31m# Ref: http://www.bicyclecards.com/how-to-play/blackjack/\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnatural\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatural\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;31m# Start the first game\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# Number of \u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseeding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# hit: add a card to players hand and return\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mis_bust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# stick: play out the dealers hand, and score\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mwhile\u001b[0m \u001b[0msum_hand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdealer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdealer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdealer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnatural\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_natural\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum_hand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdealer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musable_ace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdealer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_hand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_hand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;31m# Auto-draw another card if the score is less than 12\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mwhile\u001b[0m \u001b[0msum_hand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFile:\u001b[0m           ~/Documents/Master_AI/Year_2/Reinforcement_Learning/Uva_RLcourse_2023/lab2/blackjack.py\n","\u001b[0;31mType:\u001b[0m           type\n","\u001b[0;31mSubclasses:\u001b[0m     \n"]}],"source":["??BlackjackEnv"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-ae161126d3cb1b7b","locked":true,"schema_version":1,"solution":false}},"source":["A very simple policy for Blackjack is to *stick* if we have 20 or 21 points and *hit* otherwise. We want to know how good this policy is. This policy is *deterministic* and therefore a function that maps an observation to a single action. Technically, we can implement this as a dictionary , a function or a class with a function, where we use the last option. Moreover, it is often useful (as you will see later) to implement a function that returns  the probability $\\pi(a|s)$ for the state action pair (the probability that this policy would perform certain action in given state). We group these two functions in a policy class. To get started, let's implement this simple policy for BlackJack."]},{"cell_type":"code","execution_count":8,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-9fdcb503df9cdb08","locked":false,"schema_version":1,"solution":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["Appending to mc_autograde.py\n"]}],"source":["%%execwritefile -a mc_autograde.py\n","\n","class SimpleBlackjackPolicy(object):\n","    \"\"\"\n","    A simple BlackJack policy that sticks with 20 or 21 points and hits otherwise.\n","    \"\"\"\n","    def get_probs(self, states, actions):\n","        \"\"\"\n","        This method takes a list of states and a list of actions and returns a numpy array that contains a probability\n","        of perfoming action in given state for every corresponding state action pair. \n","\n","        Args:\n","            states: a list of states.\n","            actions: a list of actions.\n","\n","        Returns:\n","            Numpy array filled with probabilities (same length as states and actions)\n","        \"\"\"\n","        # YOUR CODE HERE\n","        \n","        probs = []\n","\n","        for state, action in zip(states, actions):\n","\n","            if state[0] >= 20:\n","\n","                if action == 0:\n","                    probs.append(1)\n","\n","                else:\n","                    probs.append(0)\n","            else:\n","\n","                if action == 0:\n","                    probs.append(0)\n","                    \n","                else:\n","                    probs.append(1)\n","        \n","        # End Code\n","        return np.array(probs)\n","    \n","    def sample_action(self, state):\n","        \"\"\"\n","        This method takes a state as input and returns an action sampled from this policy.  \n","\n","        Args:\n","            state: current state\n","\n","        Returns:\n","            An action (int).\n","        \"\"\"\n","        # YOUR CODE HERE\n","        \n","        if state[0] >= 20:\n","            action = 0\n","\n","        else:\n","            action = 1  \n","            \n","        # End Code\n","        return action"]},{"cell_type":"code","execution_count":9,"metadata":{"nbgrader":{"grade":true,"grade_id":"cell-99f02e2d9b338a5b","locked":true,"points":1,"schema_version":1,"solution":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["State: (12, 7, False)\n","Sampled Action: 1\n","Probabilities [stick, hit]: [0 1]\n"]}],"source":["# Let's check if it makes sense\n","env = BlackjackEnv()\n","s = env.reset()\n","policy = SimpleBlackjackPolicy()\n","print(\"State: {}\\nSampled Action: {}\\nProbabilities [stick, hit]: {}\".format(s, policy.sample_action(s), policy.get_probs([s,s],[0,1])))"]},{"cell_type":"markdown","metadata":{},"source":["Since there are multiple algorithms which require data from single episode (or multiple episodes) it is often useful to write a routine that will sample a single episode. This will save us some time later. Implement a *sample_episode* function which uses environment and policy to sample a single episode."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Appending to mc_autograde.py\n"]}],"source":["%%execwritefile -a mc_autograde.py\n","\n","def sample_episode(env, policy):\n","    \"\"\"\n","    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n","    and dones from environment's step function and policy's sample_action function as lists.\n","\n","    Args:\n","        env: OpenAI gym environment.\n","        policy: A policy which allows us to sample actions with its sample_action method.\n","\n","    Returns:\n","        Tuple of lists (states, actions, rewards, dones). All lists should have same length. \n","        Hint: Do not include the state after the termination in the list of states.\n","    \"\"\"\n","    states = []\n","    actions = []\n","    rewards = []\n","    dones = []\n","    \n","    # YOUR CODE HERE\n","    \n","    state = env.reset()\n","    done = False\n","    \n","    while not done:\n","            \n","        states.append(state)\n","        action = policy.sample_action(state)\n","        actions.append(action)\n","        state, reward, done, _ = env.step(action)\n","        rewards.append(reward)\n","        dones.append(done)\n","\n","    # End Code\n","\n","    return states, actions, rewards, dones"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Episode 0:\n","States [(13, 3, False)]\n","Actions [1]\n","Rewards [-1]\n","Dones [True]\n","\n","Episode 1:\n","States [(20, 10, False)]\n","Actions [0]\n","Rewards [0]\n","Dones [True]\n","\n","Episode 2:\n","States [(20, 3, True)]\n","Actions [0]\n","Rewards [1]\n","Dones [True]\n","\n"]}],"source":["# Let's sample some episodes\n","env = BlackjackEnv()\n","policy = SimpleBlackjackPolicy()\n","for episode in range(3):\n","    trajectory_data = sample_episode(env, policy)\n","    print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-0184f4c719afb98c","locked":true,"schema_version":1,"solution":false}},"source":["Now implement the MC prediction algorithm (either first visit or every visit). Hint: you can use `for i in tqdm(range(num_episodes))` to show a progress bar. Use the sampling function from above to sample data from a single episode."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Appending to mc_autograde.py\n"]}],"source":["%%execwritefile -a mc_autograde.py\n","\n","def mc_prediction(env, policy, num_episodes, discount_factor=1.0, sampling_function=sample_episode):\n","    \"\"\"\n","    Monte Carlo prediction algorithm. Calculates the value function\n","    for a given policy using sampling.\n","    \n","    Args:\n","        env: OpenAI gym environment.\n","        policy: A policy which allows us to sample actions with its sample_action method.\n","        num_episodes: Number of episodes to sample.\n","        discount_factor: Gamma discount factor.\n","        sampling_function: Function that generates data from one episode.\n","    \n","    Returns:\n","        A dictionary that maps from state -> value.\n","        The state is a tuple and the value is a float.\n","    \"\"\"\n","\n","    # Keeps track of current V and count of returns for each state\n","    # to calculate an update.\n","    V = defaultdict(float)\n","    returns_count = defaultdict(float)\n","    \n","    # YOUR CODE HERE\n","    \n","    for i in range(num_episodes):\n","\n","        states, actions, rewards, dones = sampling_function(env, policy)\n","\n","        G = 0\n","\n","        for t in reversed(range(len(states))):\n","\n","            G = discount_factor * G + rewards[t]\n","            state = states[t]\n","\n","            if state not in states[:t]:\n","                returns_count[state] += 1\n","                V[state] += (G - V[state]) / returns_count[state]\n","    \n","    # End Code\n","    return V"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["defaultdict(<class 'float'>, {(18, 1, False): -0.7454545454545455, (18, 1, True): -0.6666666666666666, (20, 4, False): 0.7121212121212123, (17, 4, False): -0.618181818181818, (13, 10, False): -0.5953002610966058, (21, 3, False): 0.9069767441860465, (15, 3, False): -0.45631067961165045, (16, 5, False): -0.6355140186915886, (14, 5, False): -0.5263157894736846, (12, 5, False): -0.6704545454545452, (20, 7, False): 0.7037037037037037, (15, 7, False): -0.5631067961165048, (16, 10, False): -0.6741071428571428, (19, 5, False): -0.703703703703704, (21, 10, True): 0.894179894179894, (21, 4, False): 0.8493150684931509, (16, 6, False): -0.6206896551724134, (19, 10, False): -0.7758620689655173, (14, 10, False): -0.6441860465116279, (12, 10, False): -0.6046511627906974, (18, 10, True): -0.6363636363636361, (15, 6, False): -0.5789473684210524, (12, 6, False): -0.45977011494252856, (18, 4, False): -0.7358490566037734, (18, 4, True): -0.7894736842105263, (17, 10, False): -0.6544622425629292, (16, 4, False): -0.8854166666666667, (18, 6, False): -0.5729166666666666, (18, 6, True): -0.5499999999999999, (16, 6, True): -0.08333333333333333, (15, 10, False): -0.686170212765958, (21, 6, False): 0.9189189189189186, (17, 6, False): -0.6138613861386139, (13, 6, False): -0.5217391304347827, (17, 2, False): -0.6956521739130431, (20, 9, False): 0.7600000000000001, (19, 9, False): -0.7211538461538461, (21, 5, True): 0.8717948717948718, (13, 1, False): -0.6792452830188679, (12, 9, False): -0.7176470588235292, (18, 10, False): -0.7149321266968326, (12, 4, False): -0.5769230769230769, (15, 8, False): -0.5999999999999999, (17, 5, False): -0.5454545454545453, (21, 4, True): 0.9249999999999999, (20, 1, False): 0.3012820512820513, (15, 4, False): -0.7446808510638298, (19, 1, False): -0.7480314960629918, (15, 1, False): -0.6339285714285711, (20, 5, False): 0.6402877697841731, (21, 7, True): 0.9347826086956518, (20, 7, True): 0.9166666666666666, (18, 3, False): -0.681818181818182, (14, 8, False): -0.6161616161616161, (15, 5, False): -0.5643564356435643, (17, 6, True): -0.25, (20, 10, False): 0.44391025641025633, (18, 7, True): -0.11764705882352944, (21, 1, False): 0.6527777777777778, (12, 1, False): -0.6237623762376237, (21, 2, False): 0.9642857142857145, (12, 2, False): -0.3863636363636364, (14, 3, False): -0.5309734513274337, (21, 7, False): 0.8933333333333332, (19, 8, False): -0.7428571428571429, (19, 7, False): -0.686868686868687, (13, 7, False): -0.5833333333333335, (13, 1, True): -0.7142857142857143, (14, 2, False): -0.5617977528089886, (19, 10, True): -0.48484848484848486, (12, 8, False): -0.5897435897435898, (19, 4, True): -0.049999999999999975, (16, 4, True): 0.14285714285714285, (13, 4, True): 1.3877787807814457e-17, (13, 4, False): -0.5591397849462363, (16, 5, True): 0.08333333333333331, (14, 6, False): -0.5053763440860214, (20, 6, False): 0.7100591715976333, (21, 5, False): 0.9333333333333333, (20, 4, True): 0.6956521739130436, (18, 9, False): -0.761904761904762, (18, 9, True): -0.6, (20, 8, False): 0.7388535031847134, (21, 2, True): 0.804878048780488, (13, 9, False): -0.6179775280898876, (16, 9, True): -0.6666666666666666, (18, 7, False): -0.6953125000000003, (21, 10, False): 0.8658536585365852, (19, 4, False): -0.8285714285714284, (20, 2, False): 0.7151898734177217, (15, 2, False): -0.5638297872340426, (21, 8, False): 0.9275362318840583, (14, 9, False): -0.5833333333333334, (12, 3, False): -0.4329896907216497, (20, 3, False): 0.5599999999999997, (20, 2, True): 0.9375, (18, 5, False): -0.642857142857143, (14, 1, False): -0.6860465116279066, (21, 8, True): 0.9230769230769231, (21, 9, True): 0.9387755102040813, (17, 1, False): -0.838095238095238, (16, 1, False): -0.6777777777777777, (18, 2, False): -0.5983606557377047, (17, 2, True): -0.42857142857142855, (13, 10, True): -0.31578947368421045, (12, 7, False): -0.49425287356321823, (20, 10, True): 0.3484848484848484, (20, 6, True): 0.3333333333333333, (17, 3, False): -0.6091954022988506, (19, 2, False): -0.6422018348623854, (18, 2, True): -0.5294117647058824, (13, 3, False): -0.4705882352941176, (14, 7, False): -0.6702127659574466, (18, 8, False): -0.6822429906542057, (19, 1, True): -0.5333333333333333, (19, 3, False): -0.6371681415929202, (17, 9, False): -0.6238532110091742, (16, 2, False): -0.6330275229357794, (16, 9, False): -0.692307692307692, (15, 9, False): -0.488095238095238, (21, 9, False): 0.9848484848484849, (16, 10, True): -0.5306122448979592, (17, 7, False): -0.6989247311827955, (16, 7, False): -0.7899999999999999, (20, 1, True): 0.1578947368421053, (13, 8, False): -0.6195652173913045, (16, 3, False): -0.7075471698113208, (16, 3, True): -0.3333333333333333, (17, 10, True): -0.4629629629629629, (16, 2, True): -0.33333333333333326, (17, 7, True): -0.26666666666666666, (21, 3, True): 0.8571428571428568, (21, 6, True): 0.8043478260869563, (13, 2, False): -0.4946236559139782, (13, 5, False): -0.666666666666667, (17, 8, False): -0.6826923076923074, (16, 8, False): -0.6581196581196578, (15, 10, True): -0.5333333333333331, (15, 8, True): -0.4444444444444444, (19, 8, True): -0.2857142857142857, (13, 8, True): -0.0833333333333333, (15, 3, True): -0.27272727272727276, (19, 6, False): -0.7232142857142856, (20, 8, True): 0.7857142857142857, (14, 4, False): -0.5888888888888889, (21, 1, True): 0.7435897435897434, (15, 7, True): -0.3999999999999999, (17, 3, True): -0.21052631578947364, (19, 3, True): -0.6842105263157895, (17, 4, True): 0.23076923076923075, (14, 4, True): -0.4, (19, 2, True): -0.7272727272727272, (14, 8, True): -0.6666666666666666, (15, 6, True): -0.08333333333333333, (18, 5, True): -0.06666666666666672, (13, 7, True): -0.2222222222222222, (12, 7, True): -0.6, (14, 5, True): -0.25, (13, 5, True): -0.4285714285714286, (18, 8, True): -0.29411764705882354, (19, 7, True): -0.23076923076923078, (14, 7, True): -0.25, (17, 5, True): -0.75, (15, 9, True): 0.09999999999999998, (12, 9, True): 0.2, (20, 9, True): 1.0, (17, 8, True): -0.7333333333333335, (14, 10, True): -0.4761904761904761, (17, 1, True): -0.5789473684210525, (15, 4, True): -0.8333333333333334, (19, 6, True): -0.23076923076923078, (14, 3, True): -0.41666666666666663, (16, 7, True): -0.33333333333333337, (18, 3, True): -0.6363636363636364, (14, 1, True): -0.5, (12, 6, True): -1.0, (12, 4, True): -0.33333333333333337, (20, 5, True): 0.43749999999999994, (14, 9, True): 0.08333333333333333, (16, 1, True): -0.6, (15, 5, True): -0.5, (14, 6, True): -0.5555555555555556, (12, 10, True): -0.5, (19, 9, True): -0.46153846153846145, (20, 3, True): 0.3, (16, 8, True): -0.4285714285714286, (15, 2, True): -0.42857142857142855, (19, 5, True): -0.41176470588235287, (13, 3, True): -0.36363636363636365, (12, 1, True): -0.5, (12, 8, True): -1.0, (12, 5, True): 0.0, (14, 2, True): 0.28571428571428575, (15, 1, True): -0.7142857142857143, (13, 6, True): -0.25000000000000006, (17, 9, True): -0.5384615384615385, (12, 3, True): -1.0, (13, 9, True): -0.8333333333333334, (13, 2, True): 0.42857142857142855, (12, 2, True): 0.0})\n"]}],"source":["V_10k = mc_prediction(env, SimpleBlackjackPolicy(), num_episodes=10000)\n","print(V_10k)"]},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-9d32f907f180c088","locked":true,"schema_version":1,"solution":false}},"source":["Now make *4 plots* like Figure 5.1 in the book. You can either make 3D plots or heatmaps. Make sure that your results look similar to the results in the book. Give your plots appropriate titles, axis labels, etc."]},{"cell_type":"code","execution_count":14,"metadata":{"nbgrader":{"grade":false,"grade_id":"cell-cbaf4d6a0e4c00fa","locked":true,"schema_version":1,"solution":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 49.3 s, sys: 269 ms, total: 49.6 s\n","Wall time: 49.3 s\n"]}],"source":["%%time\n","# Let's run your code one time\n","V_10k = mc_prediction(env, SimpleBlackjackPolicy(), num_episodes=10000)\n","V_500k = mc_prediction(env, SimpleBlackjackPolicy(), num_episodes=500000)"]},{"cell_type":"code","execution_count":15,"metadata":{"nbgrader":{"grade":true,"grade_id":"cell-ba046443478aa517","locked":false,"points":2,"schema_version":1,"solution":true}},"outputs":[],"source":["# YOUR CODE HERE\n","\n","# Make 4 plots \n","# End Code"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Off-policy Monte Carlo prediction\n","In real world, it is often beneficial to learn from the experience of others in addition to your own. For example, you can probably infer that running off the cliff with a car is a bad idea if you consider what \"return\" people who have tried it received.\n","\n","Similarly, we can benefit from the experience of other agents in reinforcement learning. In this exercise we will use off-policy monte carlo to estimate the value function of our target policy using the experience from a different behavior policy. Our target policy will be the simple policy defined above (stick if we have *20* or *21* points) and we will use a random policy that randomly chooses to stick or hit (both with 50% probability) as a behavior policy. As a first step, implement a random BlackJack policy."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Appending to mc_autograde.py\n"]}],"source":["%%execwritefile -a mc_autograde.py\n","\n","class RandomBlackjackPolicy(object):\n","    \"\"\"\n","    A random BlackJack policy.\n","    \"\"\"\n","    def get_probs(self, states, actions):\n","        \"\"\"\n","        This method takes a list of states and a list of actions and returns a numpy array that contains \n","        a probability of perfoming action in given state for every corresponding state action pair. \n","\n","        Args:\n","            states: a list of states.\n","            actions: a list of actions.\n","\n","        Returns:\n","            Numpy array filled with probabilities (same length as states and actions)\n","        \"\"\"\n","        # YOUR CODE HERE\n","        raise NotImplementedError\n","        return probs\n","    \n","    def sample_action(self, state):\n","        \"\"\"\n","        This method takes a state as input and returns an action sampled from this policy.  \n","\n","        Args:\n","            state: current state\n","\n","        Returns:\n","            An action (int).\n","        \"\"\"\n","        # YOUR CODE HERE\n","        raise NotImplementedError\n","        return action\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"ename":"NotImplementedError","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23901/1012020966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomBlackjackPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"State: {}\\nSampled Action: {}\\nProbabilities [stick, hit]: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23901/3068290644.py\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \"\"\"\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: "]}],"source":["# Let's check if it makes sense\n","env = BlackjackEnv()\n","s = env.reset()\n","policy = RandomBlackjackPolicy()\n","print(\"State: {}\\nSampled Action: {}\\nProbabilities [stick, hit]: {}\".format(s, policy.sample_action(s), policy.get_probs([s,s],[0,1])))"]},{"cell_type":"markdown","metadata":{},"source":["Now implement the MC prediction algorithm with ordinary importance sampling. Use the sampling function from above to sample data from a single episode.\n","\n","Hint: Get probs functions may be handy. You can use `for i in tqdm(range(num_episodes))` to show a progress bar."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%execwritefile -a mc_autograde.py\n","\n","def mc_importance_sampling(env, behavior_policy, target_policy, num_episodes, discount_factor=1.0,\n","                           sampling_function=sample_episode):\n","    \"\"\"\n","    Monte Carlo prediction algorithm. Calculates the value function\n","    for a given target policy using behavior policy and ordinary importance sampling.\n","    \n","    Args:\n","        env: OpenAI gym environment.\n","        behavior_policy: A policy used to collect the data.\n","        target_policy: A policy which value function we want to estimate.\n","        num_episodes: Number of episodes to sample.\n","        discount_factor: Gamma discount factor.\n","        sampling_function: Function that generates data from one episode.\n","    \n","    Returns:\n","        A dictionary that maps from state -> value.\n","        The state is a tuple and the value is a float.\n","    \"\"\"\n","\n","    # Keeps track of current V and count of returns for each state\n","    # to calculate an update.\n","    V = defaultdict(float)\n","    returns_count = defaultdict(float)\n","    \n","    # YOUR CODE HERE\n","    raise NotImplementedError\n","    return V"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["V_10k = mc_importance_sampling(env, RandomBlackjackPolicy(), SimpleBlackjackPolicy(), num_episodes=10000)\n","print(V_10k)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","# Let's run your code one time\n","V_10k = mc_importance_sampling(env, RandomBlackjackPolicy(), SimpleBlackjackPolicy(), num_episodes=10000)\n","V_500k = mc_importance_sampling(env, RandomBlackjackPolicy(), SimpleBlackjackPolicy(), num_episodes=500000)"]},{"cell_type":"markdown","metadata":{},"source":["Plot the V function. Do the plots look like what you expected?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# YOUR CODE HERE\n","raise NotImplementedError"]},{"cell_type":"markdown","metadata":{},"source":["If you want to test/submit your solution **restart the kernel, run all cells and submit the mc_autograde.py file into codegrade.**"]}],"metadata":{"celltoolbar":"Create Assignment","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":2}
